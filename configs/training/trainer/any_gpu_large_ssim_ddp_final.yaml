
kwargs:
  accelerator: auto
  max_epochs: 1000
  # gradient_clip_val: 1
  # log_gpu_memory: None  # set to min_max or all for debug
  limit_train_batches: 0.3
  val_check_interval: 1.0
  # fast_dev_run: True  # uncomment for faster debug
  # track_grad_norm: 2  # uncomment to track L2 gradients norm
  log_every_n_steps: 500
  # precision: 32
  #  precision: 16
  #  amp_backend: native
  #  amp_level: O1
  # resume_from_checkpoint: /mnt/o/git/lama5/lama/experiments/conda_2022-04-24_14-37-30_train_lama-fourier_/models/last.ckpt  # override via command line trainer.resume_from_checkpoint=path_to_checkpoint
  # resume_from_checkpoint:   # override via command line trainer.resume_from_checkpoint=path_to_checkpoint
  # terminate_on_nan: False
  # auto_scale_batch_size: True  
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 8
  limit_val_batches: 0.25
  use_distributed_sampler: False

fit_kwargs:
  ckpt_path: null
  # ckpt_path: /mnt/o/git/lama5/lama/experiments/train_lama-fourier_/models/epoch=6-step=16800.ckpt

checkpoint_kwargs:
  verbose: True
  save_top_k: 5
  save_last: True
  every_n_epochs: 1
  monitor: val_ssim_fid100_f1_total_mean
  mode: max